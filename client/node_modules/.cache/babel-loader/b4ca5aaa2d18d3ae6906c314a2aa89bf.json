{"ast":null,"code":"/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  DFAState\n} = require('./../dfa/DFAState');\n\nconst {\n  ATNConfigSet\n} = require('./ATNConfigSet');\n\nconst {\n  getCachedPredictionContext\n} = require('./../PredictionContext');\n\nconst {\n  Map\n} = require('./../Utils');\n\nclass ATNSimulator {\n  constructor(atn, sharedContextCache) {\n    /**\n     * The context cache maps all PredictionContext objects that are ==\n     * to a single cached copy. This cache is shared across all contexts\n     * in all ATNConfigs in all DFA states.  We rebuild each ATNConfigSet\n     * to use only cached nodes/graphs in addDFAState(). We don't want to\n     * fill this during closure() since there are lots of contexts that\n     * pop up but are not used ever again. It also greatly slows down closure().\n     *\n     * <p>This cache makes a huge difference in memory and a little bit in speed.\n     * For the Java grammar on java.*, it dropped the memory requirements\n     * at the end from 25M to 16M. We don't store any of the full context\n     * graphs in the DFA because they are limited to local context only,\n     * but apparently there's a lot of repetition there as well. We optimize\n     * the config contexts before storing the config set in the DFA states\n     * by literally rebuilding them with cached subgraphs only.</p>\n     *\n     * <p>I tried a cache for use during closure operations, that was\n     * whacked after each adaptivePredict(). It cost a little bit\n     * more time I think and doesn't save on the overall footprint\n     * so it's not worth the complexity.</p>\n     */\n    this.atn = atn;\n    this.sharedContextCache = sharedContextCache;\n    return this;\n  }\n\n  getCachedContext(context) {\n    if (this.sharedContextCache === null) {\n      return context;\n    }\n\n    const visited = new Map();\n    return getCachedPredictionContext(context, this.sharedContextCache, visited);\n  }\n\n} // Must distinguish between missing edge and edge we know leads nowhere///\n\n\nATNSimulator.ERROR = new DFAState(0x7FFFFFFF, new ATNConfigSet());\nmodule.exports = ATNSimulator;","map":{"version":3,"sources":["/home/mario/Desktop/ChessLion/client/node_modules/antlr4/src/antlr4/atn/ATNSimulator.js"],"names":["DFAState","require","ATNConfigSet","getCachedPredictionContext","Map","ATNSimulator","constructor","atn","sharedContextCache","getCachedContext","context","visited","ERROR","module","exports"],"mappings":"AAAA;AACA;AACA;AACA;AAEA,MAAM;AAACA,EAAAA;AAAD,IAAaC,OAAO,CAAC,mBAAD,CAA1B;;AACA,MAAM;AAACC,EAAAA;AAAD,IAAiBD,OAAO,CAAC,gBAAD,CAA9B;;AACA,MAAM;AAACE,EAAAA;AAAD,IAA+BF,OAAO,CAAC,wBAAD,CAA5C;;AACA,MAAM;AAACG,EAAAA;AAAD,IAAQH,OAAO,CAAC,YAAD,CAArB;;AAEA,MAAMI,YAAN,CAAmB;AACfC,EAAAA,WAAW,CAACC,GAAD,EAAMC,kBAAN,EAA0B;AACjC;AACR;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACQ,SAAKD,GAAL,GAAWA,GAAX;AACA,SAAKC,kBAAL,GAA0BA,kBAA1B;AACA,WAAO,IAAP;AACH;;AAEDC,EAAAA,gBAAgB,CAACC,OAAD,EAAU;AACtB,QAAI,KAAKF,kBAAL,KAA2B,IAA/B,EAAqC;AACjC,aAAOE,OAAP;AACH;;AACD,UAAMC,OAAO,GAAG,IAAIP,GAAJ,EAAhB;AACA,WAAOD,0BAA0B,CAACO,OAAD,EAAU,KAAKF,kBAAf,EAAmCG,OAAnC,CAAjC;AACH;;AAlCc,C,CAqCnB;;;AACAN,YAAY,CAACO,KAAb,GAAqB,IAAIZ,QAAJ,CAAa,UAAb,EAAyB,IAAIE,YAAJ,EAAzB,CAArB;AAGAW,MAAM,CAACC,OAAP,GAAiBT,YAAjB","sourcesContent":["/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\n\nconst {DFAState} = require('./../dfa/DFAState');\nconst {ATNConfigSet} = require('./ATNConfigSet');\nconst {getCachedPredictionContext} = require('./../PredictionContext');\nconst {Map} = require('./../Utils');\n\nclass ATNSimulator {\n    constructor(atn, sharedContextCache) {\n        /**\n         * The context cache maps all PredictionContext objects that are ==\n         * to a single cached copy. This cache is shared across all contexts\n         * in all ATNConfigs in all DFA states.  We rebuild each ATNConfigSet\n         * to use only cached nodes/graphs in addDFAState(). We don't want to\n         * fill this during closure() since there are lots of contexts that\n         * pop up but are not used ever again. It also greatly slows down closure().\n         *\n         * <p>This cache makes a huge difference in memory and a little bit in speed.\n         * For the Java grammar on java.*, it dropped the memory requirements\n         * at the end from 25M to 16M. We don't store any of the full context\n         * graphs in the DFA because they are limited to local context only,\n         * but apparently there's a lot of repetition there as well. We optimize\n         * the config contexts before storing the config set in the DFA states\n         * by literally rebuilding them with cached subgraphs only.</p>\n         *\n         * <p>I tried a cache for use during closure operations, that was\n         * whacked after each adaptivePredict(). It cost a little bit\n         * more time I think and doesn't save on the overall footprint\n         * so it's not worth the complexity.</p>\n         */\n        this.atn = atn;\n        this.sharedContextCache = sharedContextCache;\n        return this;\n    }\n\n    getCachedContext(context) {\n        if (this.sharedContextCache ===null) {\n            return context;\n        }\n        const visited = new Map();\n        return getCachedPredictionContext(context, this.sharedContextCache, visited);\n    }\n}\n\n// Must distinguish between missing edge and edge we know leads nowhere///\nATNSimulator.ERROR = new DFAState(0x7FFFFFFF, new ATNConfigSet());\n\n\nmodule.exports = ATNSimulator;\n"]},"metadata":{},"sourceType":"script"}